{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_linear_models\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers the algorithms machine learning models use to train on data.\n",
    "\n",
    "First is a linear regression model, one of the simplest models there is. There are two very different ways to train it.\n",
    "- Using a direct \"closed-form\" equation that directly computes the model parameters that best fit the model to the training set (i.e. the model parameters that minimize the cost function over the training set).\n",
    "- Using an iterative optimization approach, called Gradient Descent (GD) that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method. There are a few variants of Gradient Descent that will be uses again and again in neural networks: Batch GD, Mini-batch GD, and Stochastic GD.\n",
    "\n",
    "Next is a Polynomial Regression, a more complex model that can fit non-linear datasets. Since this model has more parameters than Linear Regression, it is more prone to overfitting the training data; there are methods to detect whether or not this is the case, using learning curves. There are also several regularization techniques that can reduce the risk of overfitting the training set.\n",
    "\n",
    "Lastly are Logistic Regression and Softmax Regression, methods commonly used for classification tasks.\n",
    "\n",
    "> Most of these equations use linear algebra and calculus. To understand the equations requires knowledge of vectors and matrices, how to transpose them, multiply them, and inverse them, and what partial derivatives are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "A Linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the *bias term* (also called the *intercept term*). The equation is as follows:\n",
    "\n",
    "$$\\hat{y} = \\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+\\dots+\\theta_{n}x_{n} $$\n",
    "- $\\hat{y}$ is the predicted value\n",
    "- $n$ is the number of features\n",
    "- $x_i$ is the $i^{th}$ feature value\n",
    "- $\\theta_j$ is the $j^{th}$ model parameter (including the bias term $\\theta_0$ and the feature weights $\\theta_{1},\\theta_{2},\\dots,\\theta_{n}$\n",
    "\n",
    "This can be written much more concisely using a vectorized form:\n",
    "\n",
    "$$\\hat{y} = h_\\theta \\textbf{(x)} = \\theta \\cdot \\textbf{x}$$\n",
    "\n",
    "- $\\theta$ is the model's *parameter vector*, containing the bias term $\\theta_0$ and the feature weights $\\theta_1$ to $\\theta_n$.\n",
    "- $\\textbf{x}$ is the instance's *feature vector*, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n",
    "- $\\theta \\cdot \\textbf{x}$ is the dot product of the vector $\\theta$ and $\\textbf{x}$, which is of course equal to $\\theta_{0}x_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+\\cdot+\\theta_{n}x_n$\n",
    "- $h_{\\theta}$ is hypothesis function, using the model parameters $\\theta$.\n",
    "\n",
    "> In Machine Learning, vectors are often represented as *column vectors*, which are 2D arrays with a single column. If $\\theta$ and $\\textbf{x}$ are column vectors, then the prediction is: $\\hat{y} = \\theta^{\\textbf{T}}_{\\textbf{x}}$, where $\\theta^{T}$ is the *transpose* of $\\theta$ (a row vector instead of a column vector) and $\\theta^{\\textbf{T}}_{\\textbf{x}}$ is the matrix multiplication of $\\theta^{\\textbf{T}}$ and $\\textbf{x}$. If is of course the same prediction, except it is now represented as a single cell matrix rather than a scalar value. In this book we will use this notation to avoid switching between dot products and matrix multiplication.\n",
    "\n",
    "When training this model, the parameters need to be set so that the model best fits the training set. First there needs to be a measure of how well (or poorly) the model fits the training data. The most common performance measure of a regression model is the Root Square Error (RMSE). Therefor, to train a Linear Regression model, you need to find the value of $\\theta$ that minimizes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "The MSE of a Linear Regression hypothesis $h_\\theta$ on a training set $\\textbf{X}$ is calculated using the equation below.\n",
    "\n",
    "$$\\text{MSE}(\\textbf{X},h_{\\theta})= \\frac{1}{m}\\sum_{i=1}^{m}(\\theta^{T}\\mathbf{x}^{(i)}-y^{(i)})^2$$\n",
    "\n",
    "Instead of an $h$ there is a $h_\\theta$  in order to make it clear that the model is parameterized by the vector $\\theta$. To simplify notation, MSE($\\theta$) will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Equation\n",
    "To find the value of $\\theta$ that minimizes the cost function, there is a *closed-form solution* - in other words, a mathematical equation that gives the result directly. This is called the *Normal Equation*\n",
    "\n",
    "$$\\hat{\\theta} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "- $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.\n",
    "- $\\textbf{y}$ is the vector of target values containing $y^{(1)}$ to $y^{m}$.\n",
    "\n",
    "Below is code that generates some linear-looking data to test this equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure generated_data_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"generated_data_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\theta}$ can be computed using the Normal Equation. Use the `inv()` function from NumPy's Linear Algebra module `(np.linalg)` to compute the inverse of a matrix, and the `dot()` method for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual function used to generate the data $y=4+3x_1+ \\text{Gaussian noise}$. Below is what the equation has found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.86501051],\n",
       "       [3.13916179]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best outcome is would have been if $\\theta_0=4$ and $\\theta_1=3$ instead of $\\theta_0=3.865$ and $\\theta_1=3.139$. This is close enough, but the noise made it impossible to recover the exat parameters of the original function.\n",
    "\n",
    "Now predictions can be made using $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.86501051],\n",
       "       [10.14333409]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict\n",
    "#should be 4, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regressions ins Scikit-Learn are simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.86501051]), array([[3.13916179]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.86501051],\n",
       "       [10.14333409]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (least squares), which can be called directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.86501051],\n",
       "       [3.13916179]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes $\\hat{\\theta} = \\textbf{X}^{+}\\textbf{y}$, where $\\textbf{X}^+$ is the *pseudoinverse* of $\\textbf{X}$ (specifically the Moore-Penrose inverse). This can be computed directly with `np.linalg.pinv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.86501051],\n",
       "       [3.13916179]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinverse itself is computed using a standard matrix factorization technique called *Singular Value Decomposition* (SVD) that can decompose the training set $\\textbf{X}$ into the matrix multiplication of three matrices $\\textbf{U}$ $\\textbf{Σ}$ $\\textbf{V}^t$ (see `numpy.linalg.svd())`. The pseudoinverse is computed as $\\textbf{X}^{+}=\\textbf{V}\\textbf{Σ}^{+}\\textbf{U}^t$. To compute the matrix $\\textbf{Σ}^{+}$, the algorithm takes $\\textbf{Σ}$ and sets to zero all values smaller than a tiny threshold value, then it replaces all the noon-zero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: the Normal Equation may not work if the matrix $\\textbf{X}^T \\textbf{X}$ is not invertible (i.e. singular), such as if $m<n$ or if some features are redundant, but the pseudoinverse is always defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "The Normal Equation computes the inverse of $\\textbf{X}^T \\textbf{X}$, which is an $(n+1) \\times (n+1)$ matrix (where $n$ is the number of features). The *computational complexity* of inverting such a matrix is typically about $O(n^{2.4})$ to $O(n^{3})$ (depending on the implementation). In other words, if the number of features are doubled, the computation time is multiplied by roughly $2^{2.4}=5.3$ to $2^{3}=8$.\n",
    "\n",
    "The SVD approach used by Scikit-Learn's `LinearRegression` class is about $O(n^{2})$. If you double the number of features, you multiply the computation time by roughly 4.\n",
    "\n",
    "> Both the Normal Equation and the SVD approach get very slow when the number of features grows large (e.g. 100,000). On the positive side, both are linear with regards to the number of instances in the training set (they are $O(m)$), so they handle large training sets efficiently, provided they can fit in memory.\n",
    "\n",
    "Once a Linear Regression model is trained (using the Normal Equation or any other algorithm), predictions are very fast: the computational complexity is linear with regards to both the number of instances you want to make predictions on and the number of features. In other words, making predictions on twice as many instances (or twice as many features) will just take roughly twice as much time.\n",
    "\n",
    "There are other different ways to train Linear Regression models better suited for cases where there are a large number of features, or too many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "*Gradient Descent* is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "Gradient Descent measures the local gradient of the  error function with regards to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero, the minimum has been reached.\n",
    "\n",
    "Concretely, the process starts by filling $\\theta$ with random values (this is called *random initilization$), and then improving it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm *converges* to a minimum. This process is illustrated below:\n",
    "\n",
    "![title](GradientDescent.png)\n",
    "\n",
    "An important parameter in Gradient Descent is the size of the steps, determined by the *learning rate* hyperparameter. If the learning rate is too small, then the algorithm will have to go thorugh many iterations to converge, which will take a long time. On the other hand, if the learning rate is too high, it might jump across the valley and end up on the other side, possibly even higher up than the previous iteration. This might make the algorithm diverge, with larger and larger values, failing to find a good solution.\n",
    "\n",
    "![title](GradientStep.jpeg)\n",
    "\n",
    "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult. The images below illustrates the two main challenges with Gradient Descent: if the random initialization starts the algorithm on the left, it will converge to a *local minimum*, which is not as good as the *global minimum*. If it starts on the right, then it will take a very long time to cross the plateau, and if it stops too early it will never reach the global minimum.\n",
    "\n",
    "![title](GradientLocal.png)\n",
    "\n",
    "The MSE cost function for a Linear Regression model happens to be a *convex function*, which means that for any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close to the global minimum (given enough time and if the learning rate is not too high)\n",
    "\n",
    "The cost function has the shape of a bowl, bit it can be elongated bowl if the features have very different scales. The figure below shows Gradient Descent on a training set where features 1 and 2 have the same scale (on the left) , and on a training set where feature 1 has much smaller values than feature 2 (on the right).\n",
    "\n",
    "![title](GDFeatureScale.png)\n",
    "\n",
    "On the left, the Gradient Descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction of the global minimum, and it ends with a long march down an almost flat valley. It will eventually reach the minimum, but it will take a long time.\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
